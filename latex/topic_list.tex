% https://de.overleaf.com/learn/latex/How_to_Write_a_Thesis_in_LaTeX_(Part_1)%3A_Basic_Structure
% https://en.wikibooks.org/wiki/LaTeX/Document_Structure
%\documentclass[11pt, twoside]{report}
\documentclass[11pt, a4paper]{report}
%\documentclass[a4paper,11pt]{scrartcl}
\usepackage{syntonly}
%\syntaxonly
\usepackage[utf8]{inputenc}
\usepackage[backend=biber]{biblatex}
\addbibresource{mybib.bib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{float}
\graphicspath{ {images/} }
\usepackage{blindtext}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{IEEEtrantools}
%\usepackage{showframe}
%\usepackage[a4paper, width=150mm, top=5mm, bottom=5mm]{geometry}
\usepackage[a4paper, width=150mm, top=25mm, bottom=25mm]{geometry}
\usepackage{fancyhdr}
%\setlength{\headheight}{12pt}
%\setlength{\headheight}{15.2pt}
%\pagestyle{empty}
%\pagestyle{myheadings}
%\pagestyle{headings}
%\pagestyle{plain}
\pagestyle{fancy}
\fancyhf{}
%\fancyhead[L]{\leftmark:}
%\fancyhead[L]{\chapter~\thechapter:}
%\fancyhead[L]{\thechapter}
\fancyhead[C]{\rightmark}
%\fancyhead[R]{\thesection}
\fancyfoot[C]{\thepage}

\setlength{\parskip}{1.0em}
\setlength{\parindent}{1em}

\usepackage{lipsum}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{mydef}{Definition}[chapter]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[chapter]


%newcommands
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
%\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\NN}{\mathcal{N}}
%\newcommand{\B}{\{-1,1\}}
%\newcommand{\bvec}[1]{\mathbf{#1}}
%\newcommand{\bv}[1]{\mathbf{#1}}
%\newcommand{\b}[1]{\boldsymbol{#1}}
\newcommand{\bv}[1]{\boldsymbol{#1}}
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
\newcommand{\gt}{>}
\newcommand{\lt}{<}
\newcommand{\tuple}[1]{\langle #1 \rangle}

\begin{document}
%\pagestyle{fancy}
\begin{titlepage}
\begin{center}
{\includegraphics{images/MPIMG_RGB_gruen.png}}\\
\vspace*{1cm}
%\vfill
\Large
\textbf{List of topics to cover}
%\vfill

%\vspace{0.5cm}


%\normalsize
\large
With section titles and brief explantions.
\vfill
%\vspace{1.0cm}

Yiftach Kolb

Berlin, \today

\vfill
{\includegraphics{images/fu-logo_bildschirm_RGB1.jpg}}
\end{center}
\end{titlepage}

%\author{Kolb, Yiftach}
%\date{Berlin, \today}
%\title{Topic List}
%\maketitle

\chapter*{Abstract}
punkt.
punkt.
\nocite{guo2017improved}

\chapter*{Declaration}
punkt.
punkt.
\nocite{bishop2006pattern}
\nocite{serre2001matrices}
\nocite{kingma2013auto}
\nocite{lotfollahi2018generative}

\chapter*{Acknowledgement}
punkt.\cite{kingma2013auto}
punkt.
bip/bop\slash boop
$\X \x \Z \z \Y \y \W \w \mathbb{Z}$
so-so--so---so

\listoftables

\listoffigures

\tableofcontents

\chapter{Introduction} punkt. \emph{punkt}, \textit{punkt}.

\chapter{Notations and definitions, preliminary concepts}
%\section{Basic Definitions}

%\section{Basic notations}
%\section{Vectors, matrices and tensors}

\section{Tensors, shape, axes, dimension}
A scalar $x \in \R$ is an element of the field and not a vector.
It has 0 dimension, $0$ axes and shape $(,)$,
A vector $\x \in \R^n$ has $n$ dimensions and one axis. Its shape is $(n,)$.
A matrix $\X \in \R^{m \times n}$ has $mn$ dimensions, 2 axes, and shape of $(m,n)$.
The generalized entity with $0$ or more axes is called \emph{tensor}.
In Pytorch~\cite{pytorch2018pytorch} terminology dimension
is used for the number of axes but I think it is
inconsistent with the way dimension is used in mathematics with regards to vectors.

If $\x$ is a tensor, $\x_i$ represents the $i$'th "row" of $\x$,
and $\x = \{\x_1, \dots , \x_n\}$ is the \emph{row representation} or 
\emph{batch
representation} of $\x$.

\section{samples, batches, mean-sum rule}

We distinguish between two types of tensors depending on what they represents.
Let $\x \in \R^{m \times n \times l}$ be a tensor.
If we say that $\x$ is a \emph{sample} or a \emph{data point} it means it is a
single sample from our data set. If we say that it is a \emph{batch}, then 
it represent a collection of $m$ samples. In this case the first axis is the batch
axis and the rest of the axes are the sample axes.

As a rule the default reduction is summation over sample axes 
and mean over the batch axes.
For example if $\x$ is a sample, 
then $\|\x \|_1 = \sum_i \sum_j \sum_k |x_{i,j,k}|$ because it has no batch axis.
If $\x$ is a batch, then we take the mean over the first axis:
$\| \x \|_1 = \frac{1}{m} \sum_i \| \x_i \|_1 = \frac{1}{m} \sum_i \sum_j \sum_k |x_{i,j,k}|$.

The reason that we do that is that for batches, we want batches of different
sizes to be comparable so it is straight forward to take mean. For the other
axes, as we will see in the case of VAE we use the ELBO function where we have
to sum over the sample axes.

\label{meansumrule}


\section{Matrices and vectors}

The type of data we work with in this paper can be represented as vectors.
For example images of shape $(h,w,c)$ can be flattened into a single axis shape
$(h \cdot w \cdot c,)$ vector.

Throughout this paper (modulo typing errors) we use
capital bold math Latin or Greek letters ($\bv{X, \Sigma}$) to represent
matrices. To stress that we talk about matrices rather than vectors we show
product ($\times$) in the dimension, i.e $\bv{X} \in \R^{m \times n}$. Although
technically the matrix--space is the tensor product $\R^m \otimes \R^n$.

Bold small math letters ($\bv{x}$) represent usually row vectors, but in cases
where it makes sense may also represent matrices such as a batch of several
vectors (each row is a different data point). In few occasions it makes sense to
let it represent both a matrix and a vector, for example, $\bv{\sigma}$ may
represent both the covariance matrix and the variance vector of a diagonal
Gaussian distribution. Non-bold math letters ($x, \sigma$, \dots) may represent
scalar or vectors in some cases and hopefully it is clear from the context or
explicitly stated.

Since we are only dealing with real matrices the transpose and the conjugation
operators are the same ($A^T = A^*$) but over $\C$ conjugation is
usually the "natural" operation and we use it to indicate that some property is
still valid over $\C$ with conjugation.

Sometimes matrices are given in row/column/block notations inside brackets where
the elements are concatenated in a way that makes positional sense.
For example
both $(\x,\y)$ and $(\x | \y)$ represent a matrix with 2 \textbf{columns}.

As mentioned usually just $\x$ means a column vector and $\x^T$ means a row
vector but sometimes in matrix notation $\x$ represents a row when it makes
sense.
We use \textbf{curly} brackets to indicate the \textbf{row} representations of a matrix.
For example $\{\x, \y\}$ represents a matrix whose \textbf{rows} are $\x$ and $\y$
(as row vectors), which alternatively could be represented as
$(\x, \y)^T$.

$(\X,\Y)$ represents
concatenation of two matrices which implicitly means they have the same number
of rows.

Zero--blocks are indicated with $0$ or are simply left as voids. For
example $ \left( \begin{array}{c c} \bv{A} & \bv{B} \\ \bv{0} & \bv{D}
\end{array} \right) $ represents block notation of an upper--triangular matrix.

\begin{mydef}
Let $\X = \{\x_1, \dots \x_m\} \in \R^{m \times n}$
be a matrix in \textbf{row} notation. Then its \emph{squared Frobenius norm} is
\begin{equation}
\label{def:frobnorm}
\|X\|_F^2 \triangleq \text{trace}(\X \X^*) 
= \sum_{i=1}^{m} \|\x_i\|^2_2 = \sum_{i=1}^m \sum_{j=1}^n x_{ij}^2
\end{equation}
\end{mydef}



\section{Functions and maps}
see~\ref{meansumrule}
Functions are usually understood to be scalar, namely $f:\R^n \to \R$ while maps
are more general $g:\R^n \to \R^m$. When we say that a map (or function) $\phi
:\R^n \to \R^m$ is \emph{parameterized}, it means that implicitly has additional 
variables that we treat as parameters $\phi_{\w}(\x) = \phi(\x, \w)$ where $\x \in \R^n$
and $\w$ is the parameter set which we don't always specify its domain and we may not
always subscript $\phi$ with it.

In the context of neural networks, when we say \emph{linear} map, we actually mean an
\emph{affine} map.
An affine map $f(x_1 \dots x_n)$ can always as linear map with one extra variable 
which is held fixed $x_0 \equiv 1$: $f(x_0, \dots x_n) = b + a_1 x_1 + \dots a_n
x_n$. $b$ is called the \emph{bias} of the map.
\label{affinelinear}

\section{Data types}
we assume that the input data unless otherwise stated is real-valued matrix.
Rows represent \emph{samples} and columns represent \emph{variables}. We assume
that each raw is a realization of a random vector. If we have $N$ rows, then the
corresponding $N$ random vectors are assumed to be independent. So depending on
the context, when we say observation, or row, we may mean the actual observed
values, or to the random vector who was realized by said observation.

We deal with two type of concrete datasets in this thesis. One of them is
Single cell RNAseq data. This data represents gene expression levels in 
individual cells, where rows represent cells and columns represent genes.
So if we see a reading of
$0.5$ in row $2$ column $4$ in means that in cell $2$ gene $4$ has normalized
expression of $0.5$.

The other type of data is images. For example the MNIST data set contains 
greyscale $28 \times 28$ images of hand written digits.
We still think of such data set as a matrix. The first axis always represents
the samples, so each "row" represent an image. The rest of the axes represent
the image. Alternatively we can also flatten the images into one axis and think
of an image as a row vector of $28*28$ dimensions.

There could possibly be additional data matrices with information about
class or conditions. We use \emph{one-hot encoding} to represent such
information.
For example in the case of the MNIST dataset every image also comes with a label
which indicate which digit it shows. Since there are $10$ classes of digits ($0$
to $9$) the class matrix is going to have $10$ columns and each row is a one-hot
vector indicating the digit of the corresponding image.

\begin{mydef}
\label{def:datamatrix}
A \emph{data matrix} is a real--valued matrix $\bv{X} \in \R^{N \times n}$
which represent a set of $N$ $n$-dimensional data points.
The $N$ rows are also called \emph{observations} and the $n$ columns are
\emph{variables}.
\end{mydef}

\begin{mydef}
\label{def:classmatrix}
A \emph{class matrix}, or also a \emph{condition matrix}
$\bv{C} \in \R^{\N \times c}$ is simply a real matrix which
represents one-hot encoding of $c$ classes or conditions over $N$ samples.
For example if sample $i$ has class $j$, then 
$(\forall k\in 1, \dots, c) \bv{C}[i,k] = \delta_{jk}$.

We say that that $\bv{C}$ is a \emph{class probability matrix} or a \emph{relaxed
class matrix} (same with condition)
if instead of being one-hot it is a distribution matrix, namely each row is
non-negative and sums up to $1$.
\end{mydef}

Usually if the input data includes class/condition information, it comes as a
class matrix (pure one-hot) but the output (the prediction) is naturally 
probabilistic and hence is relaxed.

\subsection{Input set and target set}
Sometimes the data paired into the input data $\X$ and the
target data $\Y$, 
representing for example, samples from some unknown function $f(\x) = \y$
that we want to "learn" to represent.
In classification tasks for example, $\X$ can be for example a set of images,
and $\Y$ can be their labels.

In the case of autoencoders, the target set is also the input set and $f$ is the
identity (in this case $f$ is known but we want to learn an efficient way
to represent the data).

When our data comes in several matrices, for example it could be a single cell RNAseq data
with normalized gene expression data $\X$,  and cell types as target
matrix $\Y$. It means they have must the same number of rows. And when we speak about 
paired input/target $\x,\y$ it means some $(\x,\y) \in (\X,\Y)$ so it should be
clear they belong to the same sample (row).

%\begin{mydef}
%\label{paired data}
%Let $\X = \{\x_i\}_1^N,\quad \Y = \{\y_i\}_1^N$ be two matrices in row noataion
%with the same number of rows $N$. Then
%\begin{equation}
%(\X,\Y) \triangleq \{(\x_i,\y_i)\}_1^N
%\end{equation}
%\end{mydef}

\subsection{Probabilistic interpretation of the data}
Suppose that we have a data matrix $\X = \{\x_1, \dots , \x_N\}$.
We think of $\X$ as a set of $N$ independent samples, all drawn from the same
data distribution $\x \sim p(\x)$. We think of $\x_i$ as a realization of a
random vector which we also denote with $\x_i$. 
The random vectors $\x_i$ are independent replications of a random vector $\x$.
This is another motivation why we take mean for the batch dimension because
then $\|\X\| = \frac{1}{N} \sum_1^N \|\x_i\| \approx \E [\|\x\|]$.


\section{Linear algebra preliminary: SVD and PCA}
In the following state some facts and bring without proof what are the singular
value decomposition and the principle components of a matrix. For a
full proof see~\cite{serre2001matrices}.

%\section{SVD and PCA}
Let $\bv{X} \in \R^{N \times n}$ be a real-valued matrix
representing $N$ samples of some
$n$-dimensional data points and let
$r= \text{rank}(\bv{X}) \leq \min(n,N)$. 

$\X \X^*$ and $\X^* \X$ are both symmetric and positive semi-definite.
Their eigenvalues are non-negative, and they both have
the same positive 
eigenvalues, exactly $r$ such, which we mark
$s_1^2 \geq s_2^2 \geq \dots s_r^2 \gt 0$. The values
$s_1 \dots s_r$ are called the \emph{singular values} of $\bv{X}$.

Let $\bv{S} = 
\begin{pmatrix}
s_1 & & &\\
& s_2 & &\\
& & \ddots &\\
& & & s_r
\end{pmatrix} \in \R^{r \times r}
$

Let $\bv{U} = (\bv{u}_1 | \dots | \bv{u}_N) \in \R^{N \times N}$
be the (column) right eigenvectors of $\X \X^*$ sorted
by their eigenvalues. 
Then $\bv{U} = (\bv{U}_r, \bv{U}_k)$ 
where $\bv{U}_r = (\bv{u}_1 | \dots | \bv{u}_r) \ \in \R^{N \times r}$ are the first $r$
eigenvectors corresponding to the non-zero eigenvalues, and $\bv{U}_k$ are the
eigenvectors corresponding to the $N-r$ $0$-eigenvalues.
Similarly let 
$\bv{V}  = (\bv{V}_r, \bv{V}_k)\in \R^{n \times n}$
be the (column) right eigenvectors of $\X^* \X$, sorted
by the eigenvalues, 
where $\bv{V}_r  = (\bv{v}_1 | \dots | \bv{v}_r) \in \R^{n \times r}$ are the firs $r$
eigenvalues and $\bv{V}_k$ are the $n-r$ null-eigenvectors.

The critical observations is that $\bv{V}_r = \bv{X}^* \bv{U}_r S^{-1}$
and then $\bv{U}_r^* \X \bv{V}_r = S$.

The \textit{singular value decomposition (SVD)} of $\X$ is 

\begin{equation}
\label{eq:svd}
\X = \bv{U} \bv{D} \bv{V}^*
\end{equation}
where 
$\bv{D} =
\left(
\begin{array}{c c}
\bv{S} & \bv{0} \\
%\hline
\bv{0} & \bv{0}
\end{array}
\right) \in \R^{N \times n}
$ is diagonal.

$\bv{V}_r$ are called the \textit{(right) principal components} of $\X$.
Note that $\bv{V}_r^* \bv{V}_r = \bv{I}_r$ and that 
$\X = \X \bv{V}_r \bv{V}_r^* = (\X \bv{V}_r) \bv{V}_r^T$. If one looks at the second expression, 
it means that the each row of $\X$ is spanned by the orthogonal
basis $\bv{V}_r^T$ (because the other vectors of $\bv{V}$ are in $\text{ker}(\X)$.

More generally
For every $l \leq r$, let $\bv{V}_l \in \R^{N \times l}$ be the first $l$ components,
Then $\X\bv{V}_l \bv{V}_l^T$ is as close as we can get to $\X$ within an
$l$-dimensional subspace of $R^n$, and $\bv{V_l}$ minimizes

\begin{equation}
\label{eqn:pca}
\bv{V}_l = \text{argmin}_{\W} \{
\|\X - \X \bv{W}\bv{W}^T\|_F^2 \quad : \quad \bv{W} \in \R^{n \times l}, \bv{W}^T \bv{W} =
\bv{I}_l\}
\end{equation} 

Where $\| \cdot \|_F^2$ is simply the sum of squares of the matrix' entries.

If we consider the more general
minimization problems: 

%\begin{subequations}
\begin{IEEEeqnarray}{C}
\label{eqn:pca2}
%\begin{aligned}
\min_{\bv{E,D}}\{\|\X - \X \bv{E}\bv{D}\|_F^2 \quad : 
\quad \bv{E,D^T} \in \R^{n \times
l},\} \\
\label{eqn:pca3}
\min_{\W}\{\|\X - \X \bv{W}\bv{W}^{\dagger}\|_F^2 \quad : 
\quad \bv{W} \in \R^{n \times
l},\}
%\end{aligned}
\end{IEEEeqnarray}
%\end{subequations} 

It can be shown~\cite{plaut2018principal} that the 
last two problems~\ref{eqn:pca2},~\ref{eqn:pca3} are equivalent
and that for any solution $E,D$ it must hold that 
$\bv{D}=\bv{E}^{\dagger}$. ($\bv{D}$ is the Moore--Penrose generalized
inverse of $\bv{E}$).
Moreover,
$\bv{V}_l$ still minimizes the general problem~\ref{eqn:pca2} and for every
solution $\bv{W}$, it must hold that $\text{span}\{\bv{W}\} =
\text{span}\{\bv{V}_l\}$ (but it isn't necessarily an orthogonal matrix).

\chapter{Neural networks}

We briefly discuss here some of the basics of neural network to provide clarity
and motivation. Mostly based on~\cite{nielsen2015neural}.

\section{Universal families of parameterized maps}
If we take an expression such as $f_{a,b}(x) = ax + b$, if we hold $(a,b)$ fixed
on specific values, then we get a linear function on $x$. Every assignment of
$(a,b)$ defines a different linear function and in fact every linear function on
one dimension can be uniquely described by these $a$ and $b$. So we can say that
$\{f_{a,b}\}_{a,b \in \R}$ is a \emph{parameterization} of the class of all real
linear functions on one variable. The distinction between what are the variables and what are
the parameters is somewhat arbitrary
and in the end, $f_{a,b}(x)$ is just another way to represent 
a $3$-variable function $f(a,b,x)$. 

In general we can define one or more multivariate
functions $g : \R^{n+m} \to \R^k$ (for simplicity of the discussion lets assume 
it is defined everywhere) and partition the set of its variables into $2$. 
$g_{\w}(x) \triangleq g(\w,\x)$ where 
$\x \in \R^n, \w
\in \R^m$ and let $g_{\w}(\x) := g(w,x) \in \R^k$.


We call a class $\mathcal{F}$ of parameterized functions \emph{universal} if
every continuous function can be uniformly approximated (inside a bounded
domain) by functions of that class. The class of all linear functions is not
universal. But taking "any function" $g$ is too general. What we actually want
is a class of parameterized functions that is:
\begin{itemize}
\item{} as simple as possible to construct
\item{} derivable in both the parameters as well as the variables
\item{} can uniformly approximate any continuous function in a bounded domain
given sufficiently large set of
parameters (i.e. is universal).
\end{itemize}

However these requirements are still not enough.
For example, the class of multivariate polynomials can
uniformly approximate any function. However it may not be a good idea to try
to learn very complicated high dimensional data using polynomial representation. 
For one reason is that the number of terms (monomials) grows very rapidly with the
dimension and the degree of the polynomials: for $n$ dimensions and $m$ degrees
there are something like $\binom{m + n}{n}$ monomial terms.

We want this class of simpler functions, that are almost as simple as linear,
and yet that suits well for statistical learning. For example we want to
represent complicated functions with relatively few parameters.

One such class of functions is the feed forward neural networks,
which is the class of functions that are comprised from "neurons".

\section{Neurons}

So what a "neuron"?. 
Inspired from biology, a \emph{neuron} is a many to one ($\R^n \to \R$) parameterized
function which
"integrates" the input with a linear, or affine~(see~remark~\ref{affinelinear})
function, and 
then applies a non-linear scalar function, which we call an \emph{activation
function}.
In a sense it is the simplest
function that is not linear.
Moreover we only need one type of non-linear activation, e.g sigmoid, to
construct arbitrarily complex neural networks.
A degree 2 polynomial would be considered "less 
simple" because it applies multiple non-linear multi-variable
functions $x_i x_j\dots$.

\begin{mydef}
\label{def:activationfunction}
An \emph{activation function} $\sigma : \R \to \R$ is any one of the following functions:
$x \mapsto 1$ (constant), $x \mapsto x$ (identity)
$ x \mapsto \frac{e^x}{1 + e^x}$ (sigmoid), and $x \mapsto \max(0,x)$ (ReLU).

If $\x = (x_1, \dots x_n) \in \R^n$ then $\sigma(\x)$ is the element-wise application
$\sigma(\x) \triangleq (\sigma(x_1), \dots , \sigma(x_n))$.
\end{mydef}

In the official definition we narrowed it down to just 4 kinds but in general
there are plenty of other activation functions. Also note that these functions
have no parameters.

\begin{mydef}
\label{def:neuron}
Let $\sigma : \R \to \R$ be an activation function and let $f_{\w} : \R^n \to \R$
be a \emph{parameterized} linear function. A \emph{neuron} $\nu$ is the
parameterized function 
$\nu =  \nu_{\w} \triangleq \sigma \circ
f_{\w}$.
%In case that $n=0$ then $\w = \emptyset$, we let $\nu \triangleq \sigma$.

The parameters $\w$ are called the \emph{weights} of the neuron $\nu$.
\end{mydef}

Think of the weights of a neuron as some mutable, tunable property, some sort of memory.

\begin{figure}[!h]
\begin{framed}
\centering
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=0.4\textwidth]{./plots/neuron.gv.png}
%\label{fig:neuron1}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=0.4\textwidth]{./plots/neuron.2.gv.png}
\end{subfigure}
\caption{Two graphical descriptions of the neuron
$\sigma(w_1x_1 + w_2x_2 + b)$}. Here the bias $b$ is explicitly shown
but usually it is not depicted. In the left the variable names are explicitly
shown, while in the right they are not.
\label{fig:neuron2}
\end{framed}
\end{figure}

Connecting many neurons together can create powerful parameterized
functions which we call neural networks.
Connecting means that the output of one neuron is the input to one of the
variables of a different neuron.
In feed forward networks the information only goes in one direction (no
feedback) and as we will see it means the network is a directed acyclic graph.

\begin{mydef}
\label{def:NN}
A \emph{feed forward neural network (NN)}is a \textbf{parameterized} map $\phi$
recursively defined
follows:
\begin{enumerate}
\item{} 
Activation functions ($1$, $id$, and $\sigma$) are NNs which are called the
\emph{elementary neurons} and they have no parameters ($\w=\emptyset$).
\item{} neurons are NNs
\item{} If $\psi :\R^n \to \R^m$ is a parameterized linear map then it is a NN.
\item{} If $\psi : R^n \to \R^m$
and $\rho: \R^m \to \R^l$ are NNs and their parameter sets are disjoint then $\phi = \rho
\circ \psi$ is a NN.
\item{} if $\nu:\R^n \to \R^m$ is a NN
and if $\psi_i: \R^{k_i} \to \R^{n_i}, i =1 \dots l$ are NNs, such that
$\sum_1^l n_i = n$
%and if $\psi_1, \dots \psi_n$ are NNs,
and if the parameter set of $\nu$ is disjoint from the combined parameters of
the $\psi_i$'s then
$\phi = \nu(\psi_1, \dots, \psi_n)$ is a NN
%if the dimensions "make sense".
.
\end{enumerate}

The parameter set $\w$ is called the \emph{weights} of $\phi$.
Often we don't distinguish between the network $\phi$ and its weights, and
we identify both as $\phi$.

In the definition we made the range and domain to be the entire $\R^n$ but
it is not necessary, we just need for the composition to be valid.
\end{mydef}

Feed forward neural network 
are depicted as a directed acyclic graph where every node (with its incoming
edges) corresponds to a neuron.
You can think of figure~\ref{fig:neuron2} left as depicting the neuron "component" 
in a network, while figure~\ref{fig:neuron2} right shows a neural network
description of single neuron, comprised from elementary neurons.
If we add more neurons and add depth to the

If rule 5 of the definition is not used in the construction of $\phi$, then the
resulting network is hierarchical. Its graph can be partitioned into \emph{levels}
$l_0, l_1\dots$ and there are only directed edges between two consecutive
levels $l_i \to l_{i+1}$ (see figure~\ref{fig:nn1}).

The label inside the neuron describes its activation function.
In the diagrams, we let $\sigma$ represent the sigmoid function.
We represent the identity function either by the name of the variable ($x_1$,
$y$ etc.) it acts on or simple by 
$id$.   We let the label $1$ represent the constant function. We need the
constant function because with it we can represent any affine map as a linear
map with the first input always clamped to $1$. But connecting $1$ to every
(non-input level) neuron would clutter the graph so it is not shown in most
diagrams but still implicitly assumed.
A directed edge between neurons means that the output of the neuron at the tail
is multiplied by the edge weight and assigned to the input variable of the
neuron it connects to.
Input-level neurons (sources) have no incoming edges and they represent the
beginning of the
computation.
Output neurons (sinks) have no outgoing edges and their output is the final result of
the computation.

A Node's output is therefore only dependent on the output of its direct
ancestral nodes 
(plus the bias which is usually not shown).

\begin{figure}[!h]
\begin{framed}
\centering
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=0.8\textwidth]{./plots/neuronlayers.gv.png}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=0.8\textwidth]{./plots/neuronlayers.2.gv.png}
\end{subfigure}
\caption{The network in the top didn't use rule 5~\ref{def:NN} in the construction.
It is strictly hierarchical and there are only edges between nodes of two
consecutive layers. The one on the bottom is more general.
}
\label{fig:nn1}
\end{framed}
\end{figure}

It turns out~\cite{nielsen2015neural} that the feed forward neural networks
with a single type of non-linear activation (e.g. sigmoid) and a single hidden
layer
are "universal"; Which means that
any continuous function $f$ can be
uniformly approximated by a feed forward neural network
with a single hidden layer and Sigmoid as the non-linear
activation function.
More precisely, let $B \subseteq \R^n$ be a bounded domain. Let $f : B \to \R^m$
be continuous, and let $\epsilon \in (0,1)$. Then there is a feed forward neural
network with a single hidden layer $\phi = \phi_{\w}$ and there is some value assignment
for the parameters $\w$ such that 
$(\forall \x \in B) \|\phi(\x) - f(\x)\|_2 \lt \epsilon $. The size of that single
hidden layer (the number of parameters) depends on $f$ and $\epsilon$.

In the definitions we only used linear maps to grow the network. 
There are other types of maps which are used, most commonly are convolutions 
but the principles and the graphical description remain essentially the same.

There are additional types of parameterized functions which are used "within the
layer" such as batch normalization but we won't get into that as this is not a
thesis about neural networks per se.

\begin{figure}[!h]
\begin{framed}
\centering
\includegraphics[width=0.5\textwidth]{./plots/multilayer.gv.png}
\caption{
A graph of a hierarchical feed forward neural network where the connections 
are abstracted. Edges between layers may represent in this case a fully
connected layer (every neuron has incoming edges from all neurons of the
previous layer) but it could also be used for describing a convolution.
}
\label{fig:nn2}
\end{framed}
\end{figure}

As figure~\ref{fig:nn1} shows, 
The input layer is the where the input $(\x)$ is "fed in" and the output layer
is the final result of the evaluation $\phi(\x)$.
We call all the layers (or neurons) that are 
not in the input level or the output level "hidden"
because we don't usually know what is the input/output value in these.

\section{Loss functions}

In the claim about neural networks being "universal" in terms of approximating
function $f(\x)=\y$ with neural network $\phi(\x)$. We stated specifically
convergence in terms of $l_2$ norm
$\|\phi(\x) - \y \|_2$, but the claim holds in theory and in
practice with other types of "distance-like" functions which we call loss
functions.

Moreover we usually don't know what is the function $f$ which we try to
approximate. Rather we are given 
paired samples of input/target $(\x, \y)$ and we
try to minimize the total error.

\begin{mydef}
\label{def:lossfunc}
Let $\phi :\R^n \to \R^m$ be a neural network.
A \textit{loss function} is a differentiable
function $\mathcal{L} : \R^{m+m} \to \R$. 
With "distance-like quality".
\end{mydef}

Typically the loss function is additive on the dimension, meaning it has the
form $(\forall \y,\z \in \R^m) \mathcal{L}(\y, \z)) = \sum_{i=1}^m \psi(y_i,
z_i))$

Let $\X \in \R^{N \times n}, \Y \in \R^{N \times m}$ be the input and the target
set and let $(\x,\y)$ be a paired input/target. 
We use the loss function $\mathcal{L}$ as the target function for 
the minimization problem,
 $\min_{\w} \sum_{(\x,\y)}\mathcal{L}(\phi(\x), \y)$ where the sum goes over all
paires ($N$ rows)  (input, target).


For example
$\mathcal{L}(\y, \z)) = \|\y - \z\|_2^2 = 
\sum_i |y_i - z_i|^2$ is a one such loss function (the
square error).

So far we defined $\phi$ and $\mathcal{L}$ on single input/target data points
$\x$ and $\y$. But we are interested in minimizing the total error
$\mathcal{L}(\phi(\X),\Y)$. So first we need to state how these functions
operate on sets of samples (matrices) rather than on data points (vectors).

Usually evaluation over the entire dataset is infeasible. Instead computation is
performed on batches, which are relatively small chunks of the data.

\begin{mydef}
\label{def:batch}
Let $\bv{X} \in  \R^{N \times n}$ be a data matrix. A \emph{batch}
$\bv{x} \in \R^{b \times n}$ is any subset of $b$ rows of $\bv{X}$
(Note that in this case $\x$ represents a matrix).
\end{mydef}

Batch $\x = \{\x_1, \dots \x_b\} \in \R^{b \times n}$ (row notation)
represents a subset of $b$ samples out of the total of $N$ samples in the
dataset.
Extending $\phi$ to operate on batches is trivial.
$\phi(\x) = \{\phi(\x_i)\}$ is the matrix where $\phi$ is applied on the rows of the bath.
Given an input batch $\x$ and corresponding target batch of $\y$,
We extend the loss function to batches by averaging over the batch:
$\mathcal{L}(\phi(\x), \y) \triangleq \frac{1}{b} 
\sum_{i=1}^b \mathcal{L}(\phi(\x_i), \y_i)
$

\begin{mydef}
Let $\phi$ be a neural network as defined in~\ref{def:NN} and let $\mathcal{L}$
its associated loss function as defined in~\ref{def:lossfunc}---over vectors.
Let $\x = \{\x_1, \dots , \x_b\} \in \R^{b \times n}$ be a $b$-batch (in row
notation)
, and let $\y = \{\y_1, \dots , \y_b\} \in \R^{b \times m}$ be a corresponding
target batch.
Then $\phi$ and $\mathcal{L}$ \emph{extended} over batches are:
\begin{IEEEeqnarray}{C}
\phi(\bv{x}) \triangleq \{\phi(\bv{x}_i)\}_{i=1}^m \in \R^{b \times m}\\
\label{eq:NNbatch}
\mathcal{L}(\phi(\x), \y) \triangleq \frac{1}{b}\sum_{i=1}^b \mathcal{L}(\phi(\x_i), \y_i) \in \R
\label{eq:NNbatchloss}
\end{IEEEeqnarray}
\end{mydef}

If $\mathcal{L}$ is the square error function $\| \cdot \|_2^2$ on vectors,
then its extension to batches is $\frac{1}{b}\| \cdot \|_F^2$. The reason why we
sum and don't average over the dimensions will be cleared later when we get into
variational inference.

There is also a probabilistic way to interpret the total loss.
We assume that the data points $\x, \y$ were randomly sampled from the unknown
data distribution $P(\x, \y)$.
Then equation~\ref{eq:NNbatchloss} can be reformulated as the expected 
loss~\cite{bishop2006pattern}:

\begin{equation}
\mathcal{L}(\phi(\x), \y) \approx
\mathrm{E}_{\x,\y \sim P(\x,\y)} \mathcal{L}(\phi(\x), \y)
\label{eq:NNbatchlossE}
\end{equation}

\section{Training}
This is just a brief explanation of the basic principals. Training deep networks
is a big subject which has many challenges and obstacles and a lot of
heuristics are used. 

Training the neural network $\phi_w$ means finding the weights that minimize the
loss function applied on the training input/target paired sets $\X,\Y$, in other
words minimizing $\min_{w} (\mathcal{L}(\phi_w(\X),\Y)$.
Usually we can't compute efficiently $\phi$ and $\mathcal{L}$ over the entire
sets because $N$ is too large, therefore we use batches.

\begin{mydef}
Let $\phi_{\omega}$ be a neural network and $\mathcal{L}$ its associated loss
function. And let $(\X, \Y)$ be our \emph{training set} consisting of 
the data matrix $\X$ and $\Y$
the corresponding target matrix.
Then \emph{Training} of $\phi_{\omega}$ with respect to $\mathcal{L}, \X$ 
means
algorithmically approximating the minimization problem:
\begin{equation}
\label{def:training}
\min_{\omega} \mathcal{L}(\phi_{\omega}(\X), \Y)
\end{equation}
\end{mydef}

During a \emph{training step} the network is applied on a batch $(\x,\y)$. Then the
loss function is applied on the output of the network and a gradient (with relation to the
weights) is taken using the efficient backpropagation
algorithm~\cite{nielsen2015neural}.
The gradient is used for the weight update rule, which
varies depending on the specific training algorithm. Typical training algorithms
are SGD (stochastic gradient decent) and Adam~\cite{jais2019adam},
which is the one used throughout
this work.

We only need to define the network, the loss function and the specific training
algorithm. The rest (derivation, weight update etc.) is taken care for us by the
backend of the software (Pytorch~\cite{pytorch2018pytorch}) and can be regarded
as a black box.

\subsection{Training, validation and testing data sets}

The data is partitioned into disjoint sets. The training set is used for the
training of the model. The testing set is used for the final performance
assessment. Sometimes a third subset, the validation set is used for tuning and
tweaking the model during training. The point is that the model "doesn't know"
the validation data because the weights are only trained on the training set,
but the hyper-parameters are optimized based on the validation set.
For example the validation set can be used for early
stopping during the training.
We didn't use a validation subset in our tests.
Finally the assessment is performed on the 
testing set which was completely held out during the training and
hyper-parameter tuning.


\subsection{Un/Supervised learning}

In unsupervised learning one seeks to "learn" or infer the target set $\Y$ (for example
category information) from $\X$ without seeing $\Y$ during training.
For example in the case of MNIST we want to teach the model to distinguish
10 categories of images corresponding to the $10$ digits, without having access
to the digit tags in the training set.

Supervised learning means the $\Y$ target information is fully accessible (every image
is tagged with the digit it represents). This is a much simpler classification
task.

Semisupervised learning is the hybrid case of both, where the training set
includes a small portion of known paired input/targets $(\x,\y)$ while for the
rest of the training set we only have $\x$ input and need to infer $\y$.
Semisupervised learning tasks often arise in natural situations. For example
there may be a large image data set where only a portion of the images have been
manually tagged.

\section{Autoencoders}

The basic type of an autoencoder which we informally call "vanilla" autoencoder
is a neural network that tries to "learn" the identity function. Though it sounds
pointless on a first thought, the point is how we construct this network. An
autoencoder consists of two neural networks.
An encoder network maps the input into a lower dimensional so called "latent
space", and a decoder network maps the latent space back into the high
dimensional input layer.
In the case of the vanilla autoencoder the target for the loss function is the
same as the input $\Y = \X$.

\begin{figure}[!h]
\begin{framed}
\centering
\includegraphics[width=0.8\textwidth]{./plots/autoencoderNN.gv.png}
\caption{
A graphic description of a "vanilla" autoencoder.
}
\label{fig:autoencoder}
\end{framed}
\end{figure}

\begin{mydef}
\label{def:autoencoder}
An \textit{Autoencoder} (AE) is a pair 
$(\phi, \psi)$ of feed forward neural networks 
$\psi : \R^n \to \R^m, \nu : \R^m \to \R^n$.

$\psi$ is called the \emph{encoder} network, and $\nu$ is called the
\emph{decoder} network and the composition
$\phi = \nu \circ \psi$ is called the \emph{autoencoding network}. 

We call $\R^m$ or more genrally the domain of the decoder, \emph{the latent
space}, and $\R^n$ (or more generally the domain of the encoder) is called
\emph{the observed space}.

Given a batch $\x \in \R^{b \times n}$ we call $\z = \psi(\x) \in \R^{b \times m}$
the \emph{latent representation} of $\x$ or the \emph{encoding} of $\x$.
\end{mydef}

While the definition as is given is symmetric, it is assumed that $n > m$, and
therefore $\psi$ represents dimensional reduction (in other words encoding) of
the data and $\nu$ represents expansion back to original space (decoding).

The idea here is that the original high dimensional data can be embedded
in a low dimensional space by the encoder. The decoder then can reconstruct the
original data from the embedding.

There are many variations of autoencoders. For example a "denoising" autoencoder is
essentially that same model but it receives a "noisy" version of the input and
tries to reconstruct the original clean version.
We informally call the type of autoencoder of
definition~\ref{def:autoencoder} which aims to learn the identity function 
on the original input, and which is using the square error loss function, 
a "vanilla" autoencoder. 

%A typical loss function for the AE is usually the mean sum of squares (MSE)
%$\mathcal{L}(X;\phi) = \frac{1}{N}\|X - \phi(X)\|_F^2 = \frac{1}{N}\|X - D \circ
%E (X) \|_F^2$.

%Note that the MSE can be interpreted probabilitically, if we assume our input
%comes from a random diagonal Gaussian with constant unit variant, we can interpret
%$D(E(X)) = \mu_x$ and the mean square error is $\log \mathcal{N}(x ; mu_x, 1)$
%(up to some scaling factor). Where $\log \mathcal{N}(\cdot ; \mu_x, 1)$ is the 
%log-density function for diagonal Gaussian with mean $\mu_x$ and unit variance. 

\subsection{Relation between PCA and AE}
For \textbf{centered} data, meaning every variable (column of $\bv{X}$)
has a sample mean of $0$, the first $k \leq \text{rank}(\bv{X})$ principle components
$\bv{P}$ are the solution for equation~\ref{eqn:pca}; Whereas a \textbf{linear}
autoencoder solves equation~\ref{eqn:pca3}. As mentioned, it must hold that $E =
D^{\dagger}$ (the encoder must be the Moore-Penrose inverse of the decoder).

A linear autoencoder with the  square error
loss function is almost equivalent to
PCA~\cite{plaut2018principal}; At the optimum, a bottleneck space of dimension $k$
is spanned by the first $k$ principle components of the input $\bv{X}$.
In general, an AE can be seen a PCA-like, but non-linear method for
dimensionality reduction.

\begin{figure}[!h]
\begin{framed}
\centering
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=0.95\textwidth]{images/pca.umap.mnist.png}
\caption{MNIST: UMAP of the PCA}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=0.95\textwidth]{images/ae.umap.mnist.png}
\caption{MNIST: UMAP of the AE}
\end{subfigure}
\end{framed}
\caption{"vanilla" autoencoder compared with PCA, on the MNIST dataset}
\label{fig:pcavae}
\end{figure}

Figure~\ref{fig:pcavae} shows on the left a
UMAP~\cite{mcinnes2018umap} of the principle components of the testing subset of
MNIST (images of hand written digits). On the left we see a UMAP of the latent
space encoded by the encoder of a "vanilla" autoencoder with the square error loss
function. The autoencoder was
trained on the training set and didn't "see" the testing images during training.
The results appear quite similar.


%note: talk about autoencoder not being a good generative model, usually.
%for one because we don't know how to sample from the latent space
%like who is to tell if we should use uniform, or normal or whatever...

\chapter{Variance inference and variational autoencoders}
\section{Variational Inference}

Here we briefly explain the idea behind variational inference and introduce the
ELBO which is the loss function we'll use throughout this text.
For more details see~\fullcite{bishop2006pattern}.

We treat the data matrix as a set of independent observations (its rows)
$\bv{X} = \{\bv{x}_1, \dots
, \bv{x}_N\}$ which we try to explain by a probabilistic model. 
Each row $\x_i$ is thought of a realization of a random vector, which we also 
denote as $\x_i$ (as explained in the notation section) and similarly $\X$
represent both the set of r.vs as well as the realization itself.

We assume that
the $\bv{x}_i$'s are independent and identically distributes (i.i.d) random vectors
with some distribution function $\x \sim p(\x)$ and therefore for
the entire dataset it holds that $p(\bv{X}) = \prod p(\bv{x}_i)$.

\begin{mydef}
\label{def:logevidence}
Let $\bv{X} \in \R^{N \times n}$ be a data matrix and let $\{\bv{x}_i\}_1^n$ be its
rows,
which we assume to be i.i.d with some (unknown) distribution $\x \sim p(\bv{x})$.
Then $\log p(\bv{X}) = \sum_1^N \log p(\bv{x}_i)$ is called the \emph{log evidence} of our
data.

$\frac{1}{N}\log p(\X)$ is the \emph{mean log evidence} (remember the
mean-sum rule for data sets and
batches~\ref{meansumrule}).
\end{mydef}

An observation r.v $\bv{x}$ is high dimensional however
we have some reason to believe that behind the scenes there is some hidden
(latent), smaller dimensional random vector $\z$ that generates it.
%$\bv{Z} = \{\bv{z}_1 \dots \bv{z}_N\}$ that generate the observations $\bv{X}$.
In other words we think that $\x$ is conditioned on $\z$ and we can speak of
the joint distribution $p(\x,\z) = p(\x | \z)p(\z)$.
%Because we assume i.i.d for both $\bv{X}$ and $\bv{Z}$ all the distributions factor
%over the individual samples multiplicatively, e.g.
%$p(\bv{X}|\bv{Z}) = \prod p(\bv{x}_i | \bv{z}_i)$,
%$p(\bv{X}) = \prod p(\bv{x}_i)$ and so forth. This makes working with log
%probabilities very easy.

Suppose that we have a fully Bayesian model. In this case there are no
parameters because the parameters are themselves stochastic variables with some
suitable priors. We can therefore pack all the latent variables and stochastic
parameters into one latent "meta variable" $\z$,
%$\bv{Z} = (\bv{z}_1, \bv{z}_2, \dots )$,
%where each $\bv{z}_i$
which is some multidimensional r.v and possibly composed of several simpler r.vs (for
example a categorical and a normal r.vs).
We similarly pack all the observed variables into one meta variable $\bv{x}$.
Together we have a distribution $p(\bv{x},\bv{z})$ and the working assumption is that it
is easy to factorize $p(\bv{x},\bv{z}) = p(\bv{x}|\bv{z})p(\bv{z})$,
however $p(\bv{z}|\bv{x})$ is intractable and
$p(\bv{x})$ is unknown.

We are being Bayesian here so we consider $\bv{X} = (\bv{x}_1, \bv{x}_2, \dots)$ to be
a constant a set of observations and we want to best explain $p(\bv{X})$ by finding as
high as possible lower bound for it (or rather to $\log p(\bv{X})$, the \emph{log
evidence}).
A second goal is to approximate the intractable $p(\bv{z}|\bv{x})$ by some simpler
distribution $q(\bv{z})$ taken from some family of distributions.

\begin{mydef}
Let $\bv{x},\bv{z}$ be random variables with joint
distribution $p(\bv{x},\bv{z})$ and let $q(\bv{z})$ be any distribution.
Let $(\X,\Z) = \{(\x_1,\z_1), \dots , (\x_N,\z_N)\}$
be $N$ independent replications of $(\x,\z)$.
The \emph{evidence lower bound (ELBO)} with respect to $p,q$ is:
\begin{IEEEeqnarray}{C}
\label{def:elbo}
-\mathcal{L}(q,p,\x) \triangleq
\int \log \frac{p(\bv{x},\bv{z})}{q(\bv{z})} d q(\bv{z}) \\
\label{def:elboX}
%-\mathcal{L}(q,p) \triangleq - \mathcal{L}(q,p,\X) = 
%\int \log \frac{p(\bv{X},\bv{Z})}{q(\bv{Z})} d q(\bv{Z})
-\LL(q,p) \triangleq 
-\LL(q,p, \X) 
= \frac{1}{N} \sum_1^N (-\LL(q,p,\x_i) \\ 
\approx
\E_{\x} [-\LL (q,p, \x)]
\end{IEEEeqnarray}
\end{mydef}

Equation~\ref{def:elboX} is no longer treated as a function of $\X$ because it
is taken over all of our data which we think of as a constant.
The reason that we mark the ELBO with $-\LL$ is because we use the minus ELBO,
$\LL$, as the 
loss function for VAEs.

The following equation shows that the \emph{ELBO} is a lower bound for the
\emph{mean log
evidence}.
(using Jensen's inequality)

\begin{equation}
\label{eq:elbo}
\begin{aligned}
\frac{1}{N} \log p(\bv{X}) &= \frac{1}{N} \log \int p(\bv{X},\bv{Z}) d\bv{Z} 
= \frac{1}{N} \log \int \frac{p(\bv{X},\bv{Z})}{q(\bv{Z})} q(\bv{Z})d\bv{Z} \\
&=  \frac{1}{N} \log \int \frac{p(\bv{X},\bv{Z})}{q(\bv{Z})}dq(\bv{Z}) 
\geq  \frac{1}{N} \int \log \frac{p(\bv{X},\bv{Z})}{q(\bv{Z})}dq(\bv{Z}) \\
&= \frac{1}{N} \int \sum_1^N \log \frac{p(\x_i, \z_i)}{q(\z_i)}dq(\z_i) \\
&= \frac{1}{N} \sum_1^N - \LL(q,p,\x_i)
= -\mathcal{L}(q,p,X) \triangleq -\mathcal{L}(q,p)
\end{aligned}
\end{equation}

In equation~\ref{eq:elbo} we found a lower bound $-\mathcal{L}(q,p)$ for the
mean log
evidence $\log p(\bv{X})$, the \emph{ELBO}.
Whatever distribution $q$ we put in ELBO will not be
greater than the real log evidence so we are looking for the $q$ which
\textbf{maximizes} it.

Now we show that maximizing the ELBO actually obtains the log evidence and it is
equivalent to minimizing $KL(q(\bv{Z}) \| p(\bv{Z}|\bv{X})$:

\begin{equation}\label{eq:kl_bound}
\begin{aligned}
-\mathcal{L}(q,p,\x) &\triangleq \int \log \frac{p(\bv{x},\bv{z})}{q(\bv{z})} d
q(\bv{z})
= \int \log \frac{p(\bv{z}|\bv{x})p(\bv{x})}{q(\bv{z})} d q(\bv{z}) \\
&= \int \log p(\bv{x}) dq(\bv{z}) - \int \log \frac{q(\bv{z})}{p(\bv{z}|\bv{x})}
dq(\bv{z}) 
= \log p(\bv{x}) - KL(q(\bv{z}) \| p(\bv{z}|\bv{x})
\end{aligned}
\end{equation}

We can rewrite equation~\ref{eq:kl_bound} as:
\begin{equation}\label{eq:elbokl}
\log p(\bv{x}) = -\mathcal{L}(q,p,\x) - KL(q(\bv{z}) \| p(\bv{z}|\bv{x}))
\end{equation}

Equation~\ref{eq:elbokl} shows that the ELBO minus the kl-divergence are constant
and equal the log evidence. Therefore minimizing the kl-divergence (which is
always non-negative) simultaneously maximizes the ELBO and vicer-versa.

%In reality we can't search in the set of \textbf{all} possible distributions 
%$q(\bv{Z})$. Instead we limit our search to some parameterized family of simple distributions
%$\{q_\alpha(\bv{Z})\}_{\alpha}$ where $\alpha$ is the parameter set. For a simple
%one dimensional concrete example, we can consider the set of all
%Gaussian distributions $\{\mathcal{N}(\bv{z} ; \mu, \sigma)\}_{\mu,\sigma}$.
%
%The task therefore is to use the data $\bv{X}$ to find the best parameter 
%$\hat\alpha$ that maximizes $\mathcal{L}(q_{\alpha})$:
%\begin{equation}
%\label{eq:alphahat}
%\hat\alpha \triangleq \text{arg max}_{\alpha}(\mathcal{L}(q_{\alpha},p))
%\end{equation}

\section{Variational Autoencoder}
\subsection{Adding parameters}

Our models will not be fully Bayesian, but rather parametrized.
In this case let $\theta$ represent the set of parameters for $p$, and $\phi$
the parameters for $q$. Meaning we are dealing with a family of distributions
$p_{\theta}(\x,\z)$ and another family $q_{\phi}(\z)$.

For any $\theta$ and any $\phi$, the equations from the previous chapter hold
also in the parametrize form, i.e 
$\log p_{\theta}(\x) =
-\mathcal{L}(q_{\phi},p_{\theta},\x) -
KL(q_{\phi}(\z) \| p_{\theta}(\z|\x)$.

We assume that we can only approach the "real" distribution using
$\theta$ from below $\log p(\x) \geq \log p_{\theta}(\x)$.
So together with equation~\ref{eq:elbo} we have

\begin{equation}\label{eq:parelbo}
\begin{aligned}
(\forall \theta, \phi)\log p(\x) & \geq \log p_{\theta}(\x) 
\geq -\mathcal{L}(q_{\phi}.p_{\theta},\x)
= \int \frac{p_{\theta}(\x,\z)}{q_{\phi}(\z)} dq_{\phi}(\z)
\end{aligned}
\end{equation}

And from equation~\ref{eq:elbokl} we again see that by finding the parameters
$\phi$, $\theta$ that maximize the ELBO we approach the real log evidence as much
as we can within the limits of the parametrized family of distributions we use.

\subsection{Rearranging the ELBO}
Equations~\ref{eq:elbo} and~\ref{eq:kl_bound} were defined for any distribution
$q(\z)$ and in particular we are allowed to plug in a conditioned
distribution $q(\z|\x)$. That implies the existence of $q(\z,\x)$ and $q(\x)$
but we actually don't care about them. We condition everything on $\x$ but $\x$
is treated as a given constant from a Bayesian view point and we only want to
somehow make $q(\z|\x)$ to closely approximate $p(\z | \x)$.

A second thing we need to achieve is to express the -ELBO in terms of $p(\x|\z)$
and $q(\z|\x)$ rather than the joint distribution. To that end we need also the
prior $p(\z)$.

%\begin{mydef}
%The \emph{conditioned (on $\X$) ELBO} is
\begin{equation}
\begin{aligned}
\mathcal{L}(q,p,\x) &\triangleq \int -\log \frac{p(\x,\z)}{q(\z|\x)}dq(\z|\x) 
= \int -\log \frac{p(\x |\z) p(\z)}{q(\z|\x)}dq(\z|\x) \\
&= \int -\log p(\x | \z)dq(\z|\x) + \int \log \frac{q(\z|\x)}{p(\z)}dq(\z|\x) \\
&= \int -\log p(\x | \z)dq(\z|\x) + KL(q(\z|\x) \| p(\z))
\label{eq:elbo_conditioned}
\end{aligned}
\end{equation}
%\end{mydef}

So to sum it up, if we want to maximize the log evidence $\log p(\X)$ it
suffices to minimize $\mathcal{L}(q,p)$ and equation~\ref{eq:elbo_conditioned}
shows that this means finding the balance between making 
the term $\int \log p(\X | \Z)dq(\Z|\X)$ (which we call the reconstruction term) large as possible, 
and making the KL-term small.
The KL term is seen as a regularization term.

\subsection{Mean field approximation}
Usually we treat the dimensions of $\x$, $\z$ etc. as independent.
That means if $\x = (\x_1, \dots, \x_n)$ is a r.v. in $\R^n$ 
we assume that the $\x_i$ are independent and therefore 
$P(\x) = \prod_1^n P_i(\x_i)$.
Mean field approximation simplifies the implementation and speeds up the
computation and has been the standard practice since the
beginning of VAEs~\cite{kingma2013auto}.

\subsection{Using neural networks for the parametrization}
In this text we deal with variational autoencoders (VAE).
A VAE is a neural network which is used to define and optimize the parameters
$\phi$ and $\theta$ which define $p_{\theta}(\x | \z)$ and $q_{\phi}(\z | \x)$
and we train the network to maximize equation~\ref{eq:elbo_conditioned}.

Specifically the encoder part of the network is a feed forward neural network 
$f_{\theta}(\z)$ which is
used to define $P_{\theta}(x|z)$. For example, we can assume that $P_{\theta}$
is a family of multivariate Gaussians and in this case $f_{\theta}(z) =
(\mu(z), \Sigma(z))$. Meaning the encoder maps $z$ to the location vector and
covariance matrix. The parameter $\theta$ in this case are the weights of the
encoder neural network. 

The decoder network is similarly defined as neural network $g_{\phi}(\x)$ which
maps $\x$ into the parameters defining the family $q_{\phi}(\z)$. Here too $\phi$
represent the weights of the decoder.

For the prior $p(z)$ we set some fixed prior distribution.

Note that the encoder network (similarly the decoder) is used to define a distribution over
$\z \in \R^m$, but the encoder itself maps into some other space. 
For example,
to define a normal one Gaussian distribution (so over $\z \in \R$, the encoder
maps into $\R^2$, and its output creates $\mu, \sigma \R$ which are used to
define the Gaussian distribution $\mathcal{N}(\z ; \mu, \sigma)$.
We also need to make sure that the range of the network obeys to the constraints
of the parameters. For example the variance must be non-negative. 
Alternatively we can use transformations to remove constraints. For example
instead of letting the network specify the variance. we let it specify the
log-variance.

\begin{mydef}
\label{def:vae}
Let $\{p_{\theta}\}_{\theta \in \Theta}$ be a parameterized 
family of distributions over $\R^n$ and let
$\{q_{\phi}\}_{\phi \in \Phi}$ be a family of distributions over $\R^m$.
Where $\Theta$ and $\Phi$ are real domains (i.e $\Theta \subseteq \R^k \dots$).

A \emph{variational autoencoder (VAE)} consists of a pair $(E,D)$ of neural networks,
$E : \R^n \to \Phi$ and $D : \R^m \to \Theta$ and some fixed distribution $p \in
\Phi$.

We call $\R^m$ or more genrally the domain of the decoder, \emph{the latent
space}, and $\R^n$ (or more generally the domain of the encoder) is called
\emph{the observed space}.
$p$ is called \emph{the prior distribution of the latent space}.
\end{mydef}


While an autoencoder works on deterministic data, 
with the encoder mapping input $\x \mapsto \z$ and the decoder then maps the
latent space $\z \mapsto \hat{\x}$ to the reconstruction, a VAE does
basically the same thing but non-deterministically.
It maps $\x$ into a distribution over $\z$: $\x \mapsto q(\z|\x)$ and it maps
$\z$ into a distribution over $\x$, $\z \mapsto p(\x|\z)$.

The loss function associated with a VAE is minus ELBO. This means training
the VAR maximizes equation~\ref{eq:elbo_conditioned} and therefore also
the log evidence.

\subsection{Computing the ELBO}
It may not be immediately clear how to how to compute the integral in the ELBO function. 
Recall that given an input $\x \in \R^n$,
The loss function is 

\begin{equation}
\LL(p,q,\x) = 
\int - \log \frac{p(\x|\z)p(\z)}{q(\z|\x)}dq(\z|\x)
= \int -\log p(\x|\z)dq(\z|\x) + \int \log \frac{q(\z|\x)}{p(\z)}dq(\z|\x)
\label{eq:elbovae}
\end{equation}

Given concrete input $\x \in \R^n$,
the decoder specifies a distribution over
$\z \in \R^m$ rather then a concrete deterministic point:
$\z \sim q(\z | \x)$. Suppose that we draw one concrete sample $\z \in \R^m$
taken from that distribution. Now that we have the concrete input $\x$ and a 
concrete $\z$ we can compute $\log q (\z | \x)$ as well as the prior $p(z)$.
Remember that the decoder takes $\z$ and produces a distribution $p(\x | \z)$.
With a concrete $\z$, and $\x$ we can also compute $\log p(\x | \z)$
So once we draw a specific sample $\z$ we can compute everything inside the
integral.

In fact what we have done is already a form of Monte Carlo integration.
More generally, instead of drawing just one concrete sample $\z$, we draw $k$ samples
$\z_i \sim q(\z | \x)$ per input $\x$, and take the average. Then we have

\begin{equation}
\begin{aligned}
\LL(p,q,\x) = 
\int - \log \frac{p(\x|\z)p(\z)}{q(\z|\x)}dq(\z|\x) \\
= \int -\log p(\x|\z)dq(\z|\x) + 
\int \log \frac{q(\z|\x)}{p(\z)}dq(\z|\x) \\
\approx \frac{1}{k} \sum_{i=1}^k [-\log p(\x|\z_i)
+ \log \frac{q(\z_i|\x)}{p(\z_i)} ]
\label{eq:elbomc}
\end{aligned}
\end{equation}

In practice we take just one ($k=1$) sample $z$ for each input data point $x$.
Remember that we are working on batches and computing an average loss over
batches so for a given batch we are taking many samples $\z$.
Experimental data shows that taking larger samples usually brings little
benefit~\cite{kingma2019introduction}.

\subsection{using the decoder as data generator encoder for dimensionality
reduction}
given a VAE $(E, D, p)$, synthetic data samples can be generated as follows:
sample $\z \sim p(\z)$. Then given the samples in the latent space, sample
from the decoded distribution $\x \sim D(\z)$, in the observed space.

Given observation $\x$, we can non deterministically encode $\x$ into 
the latent space by taking the mean: $\x \mapsto \E [E(\x)]$. Or we can
non-deterministically draw $\z$ from the encoded distribution:
$\x \mapsto \z \sim E(\x)$.

\subsection{Choosing the distribution types}
This is of course another topic that can get arbitrarily complex.

Recall that our loss function (if we take just one sample $\z$ for intput $\x$
is:
\begin{equation}
\label{eq:vanillavaeloss}
\begin{aligned}
\LL(p,q,\x) 
= (-\int \log p (\x | \z)dq) + KL(q(\z | \x) \| p(\z)) \\
\approx -\log p (\x | \z) + \log \frac{\log q(\z | \x)}{p(\z)}
\end{aligned}
\end{equation}

The left term is called the reconstruction error and the 
right term is called the regularization term or the kl-term.


Lets suppose that we just want to use diagonal Gaussian distributions (which is
a type of mean field approximation).
The advantage is that it is easy to compute them because we just sum over the
dimensions:

\begin{equation}
\label{eq:diagnormal}
\log \NN(\x ; \bv{\mu}, \bv{\sigma}) = 
\sum_1^n \log \NN(\x_i ; \bv{\mu}_i, \bv{\sigma}_i)
\end{equation}

The dimension of the latent space is a significant hyper parameter of the VAE
model.
Since we sum over the dimensions rather than averaging,
the larger we let the dimension of the latent space $\z$ 
it can have an effect of upscaling the importance of the kl-term relative to the
reconstruction.
Moreover the dimension should also be appropriate in terms of the "real"
dimensionality of the data.

In the case of the vanilla VAE we choose $p(\z) = \NN(\z;0,1)$ (diagonal
Gaussian standard normal) for the prior and 
$q(\z | \x) = \NN(\z ; \bv{\mu}(\x), \bv{\sigma}(\x))$ for the encoder.
There is a closed form formula for KL-divergence between two diagonal Gaussians
so in this case we don't need to use Monte Carlo integration for the KL-term
(we show it in one dimensions and for $k$ dimension in the diagonal case
we sum over the dimensions:

\begin{equation}
\label{eq:kldivnormal}
KL(\NN(;\mu_1, \sigma_1) \| \NN(;\mu_2, \sigma_2)) = 
-\frac{1}{2} + \log \frac{\sigma_2}{\sigma_1}
+ \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2}
\end{equation}



The reconstruction term is more tricky.
Lets say we still want some sort of Gaussian diagonal distribution for $\x$,
so $p(\x | \z) = \NN(\x ; \bv{\mu}(\z), \bv{\sigma}(\z))$.
If we allow the decoder to create arbitrarily small variances, then the
decoder will stop being stochastic and it becomes deterministic. 
The decoder will also be able to pinpoint the sources in the latent space.
As a result the encoding becomes meaning-less and the encoded data will tend to
be arbitrarily spread in the latent space without meaningful clusters.
In addition there is a problem of numerical instability because the distribution
function can become arbitrarily large and the reconstruction loss might approach
$-\infty$.

A common approach is to use a fixed variance of $1$, because then the
reconstruction term becomes square error loss.
$p(\x | \z) = \NN(\x ; \bv{\mu}(\z), 1))$.

Another approach, $\sigma$-VAE~\cite{rybkin2021simple}, is to let the variance
be a trainable parameter but not a function of $\z$.
$p(\x | \z) = \NN(\x ; \bv{\mu}(\z), \bv{\sigma})$.
In this case as well in my experiments at least, if $\sigma$ is allowed to be
too small we sometimes get similar issues of numerical stability, 
non-stochastic decoder and meaningless 
encoding in the latent
space.

\subsection{VAE as an generalization of AE}
Suppose that we take a $\sigma$-VAE as described above, and suppose that we hold
$\sigma$ fixed.

So the decoder is $p(\x | \z) = \NN(\x ; \mu(\z), \sigma)$ where $\mu(z)$ is a a
function of $\z$ the decoder neural network. The decoder is $q(\z | \x) = \NN(\z
; \mu(\x), \sigma(\x))$ For the kl-term we use the analytical solution and for
the reconstruction error we use Monte Carlo integration with one sample. We can
assume that the KL term is uniformly (with respect to $\x$) bounded by some
constant $M$. We could also make sure that the encoder map is bounded but
normally there is no reason for the encoder to take the mean or the variance to
infinity during training as it would just increase the loss.

Our -ELBO loss function is:
\begin{equation}
\begin{aligned}
\LL(p,q, \x) = -\log \NN(\x; \mu(\z), \sigma) + KL(q(\z | \x) \| p(\z)) \\
= \frac{1}{2}\|\frac{x - \mu(\z)}{\sigma}\|^2 + \log \sigma + KL(q(\z | \x) \| p(\z))
\end{aligned}
\label{eq:losss}
\end{equation}

Minimizing $\LL(p,q,\x)$ in equation~\ref{eq:losss} is equivalent to minimizing 
$\sigma \LL(p,q,\x)$, and if we let $\sigma \to 0$ we get:
\begin{equation}
\begin{aligned}
\lim_{\sigma \rightarrow 0} \LL(p,q, \x) \sigma 
= \lim_{\sigma \to 0}[ -\log \NN(\x; \mu(\z), \sigma) \sigma + \sigma KL(q(\z |
\x) \| p(\z))] \\
= \lim_{\sigma \to 0} [\frac{1}{2}\|x - \mu(\z)\|^2 + \sigma \log \sigma  +
\sigma KL(q(\z | \x) \| p(\z))] = 
\frac{1}{2}\|x - \mu(\z)\|^2
\end{aligned}
\label{eq:lossss}
\end{equation}

So if we set $\sigma$ be arbitrary small, $p(\x | \z)$ become almost point mass
and the KL term loses significance, effectively removing the constraint from the
encoder. As a result the encoder itself
will also become point mass in order to minimize the reconstruction term and
disregarding the constraint on its distribution.
Thus the VAE becomes essentially a vanilla auto encoder at the limit.

\subsection{Graphical representation}

It is both convenient as well as informative to include a graphical description
of our probabilistic models by way of plate diagrams.

In a plate diagram nodes represent random variables and arrows
represent dependency.
Figure~\ref{fig:vae_model} is a plate diagram of the VAE model with slight
adaptation. We use doted arrows with round arrowhead to represent the inference model
(encoder network),
and regular arrows for the generative model (decoder network) so the combined
diagram describes the two networks together.
Regular (triangular) arrowhead

Plate represents packing of $N$ i.i.d random vectors since we have $N$ observations $X =
(x_i)_1^N$ and correspondingly $N$ latent variables $Z = (z_i)$.

Shaded node represent known values (either observations or the prior
distribution).

The squared $\zeta$ node represent some fixed parameter which describes the
prior distribution of $p(\z) := p(\z | \zeta)$.
It is possible to make $\zeta$ a non-fixed stochastic parameter but in the case of this
vanilla VAE I don't think it has any advantage and don't know of anyone who 
does that. 

The generative model therefore factors as: $p(\x,\z) = p(\x|\z)p(\z|\zeta) =
p(\x|\z)p(\z)$.
The inference model in this case is just $q(\z | \x)$.

\begin{figure}[!h]
\begin{framed}
\centering
\begin{subfigure}[b]{0.2\textwidth}
\includegraphics[width=\textwidth]{plots/vae_p.gv.png}
\caption{generative model ($p(x,z)$}
\end{subfigure}
\begin{subfigure}[b]{0.2\textwidth}
\includegraphics[width=\textwidth]{plots/vae_q.gv.png}
\caption{inference model ($q(z|x)$)}
\end{subfigure}
\begin{subfigure}[b]{0.2\textwidth}
\includegraphics[width=\textwidth]{plots/vae.gv.png}
\caption{the combined graphical model}
\end{subfigure}
\caption{VAE graphical model}
\label{fig:vae_model}
\end{framed}
\end{figure}

Note that the graphical model has no assumption about the specific types of
distributions involved (Gaussian, Dirichlet or whatever \dots) and that is left
for the actual implementation.

In the case of a "vanilla" VAE $(E,D,p)$, 
We use mean field approximation for $p$ and $q$ with Gaussian distributions.
We set the prior $p(\z)$ to be diagonal standard Gaussian
$p(\z) \sim \NN(;\bv{0},\bv{1})$.
And $p(\z | \x) \sim \NN(;D(\z))$ is a diagonal Gaussian, where the decoder
determines its means and variances $D(\z) = (\bv{\mu}(\z), \bv{\sigma}(\z))$, 
And similarly $q(\z | \x) \sim \NN(;E(\x))$.

\begin{figure}[!h]
\begin{framed}
\centering
\begin{subfigure}[t]{0.3\textwidth}
\includegraphics[width=\textwidth]{images/vae.umap.mnist.mu.png}
\caption{umap: means}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
\includegraphics[width=\textwidth]{images/vae.umap.mnist.sampling.png}
\caption{umap: sampling}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
\includegraphics[width=0.9\textwidth]{images/vae.generation.mnist.sampling.png}
\caption{randomly generated digits}
\end{subfigure}
\caption{VAE showing umap plots of the latent space. The first shows 
the encoding where the mean of the distribution $p(\z | \x)$ is used. The second
uses a random sample from the distribution for the encoding.}
\label{fig:vaeumap}
\end{framed}
\end{figure}

\section{Expanding the VAE model}
If we look at figure~\ref{fig:vae_model} it looks very simple, but it also
pretty much forces us to choose a simple type of distribution family (e.g
diagonal Gaussians in the case of the vanilla VAE).
Recall that That $\z$ packs up all the latent variables and the stochastic
parameters and $\x$ packs up all the observed variables.

We can describe a more complex distribution by unpacking them and describe the
dependencies between them.
This is done in the following way:
\begin{enumerate}
\item{} Define the set of observed random vectors $\x_1, \x_2, \dots \x_k$, and 
the set of latent random vectors and stochastic parameters $\z_1, \dots \z_l$.
\item{} Specify how to factor the generative model $p(\x_1,\dots, \x_k| \z_1
\dots , \z_l)$
\item{} Specify how to factor the inference model $q(\z_1 \dots \z_l | \x1,
\dots \x_k)$
\item{} Choose appropriate priors $p(z_i)$ and
\item{} Choose appropriate distribution families for the $\x_i$ and $\z_i$,
and choose priors $p(\z_i)$.
\end{enumerate}

\subsection{Example: CVAE}
Suppose that we have data that carries both numerical and categorical data $(\X,
C)$.
For example suppose that $\X$ represent a set of images (as flattened vectors),
and $C$ represents the object types shown in the images.
Moreover lets assume that we have $k$ types of categories and that the data is
balanced so we have $k/N$ samples from each category.
We have just specified our observed variables $\x, c$.
We will have one latent variable $\z$ (remember it is actually a vector but we
call them variables...).
The idea here is that because we have different categories, we will have some
type of a mixture of distributions.

Lets specify the generative model $p(\x, \z, c)$. 
We can factor it "arbitrarily" but the choice may make meaningful different in
the result. Luckily in this case there aren't too many
ways to do that.
We can factor as $p(\x, \z, c) = p(\x | \z, c)p(\z | c)p(c)$ which mean $\x$ is
directly dependent on both $\z$ and $c$.
We can opt for a model where given $\z$, $\x$ and $c$ are independent. 

As for the inference model (encoder), given the observation $\x$ and $c$ it will determine
our only latent variable $\z$, in other words $q(\z | \x, c)$ is the inference
model without anything further to factorize. We might try to remove the
dependency on $c$, and make it $q(\z | \x)$.

Since our data is balanced, it is clear that $p(c) = \frac{1}{k}$.

The generative process (decoder) is therefore as follows:
\begin{itemize}
\item{} draw a category $c \sim Cat(\frac{1}{k})$.
\item{} draw $\z \sim p(\z | c)$.
\item{} draw $\x \sim p(\x | \z)$.
\item{} the resulting factorization of $p$ is:
$p(\x, \z, c) = p(\x | \z)p(\z | c)\frac{1}{k}$.
\end{itemize}

Remember that the loss function is still the minus ELBO,
which according to our factorization becomes:
\begin{equation}
\begin{aligned}
\label{eq:cvaeloss}
\LL(p,q,\x,c) = 
\int -\log \frac{p(\x,c,\z)}{q(\z | \x, c)}dq \\
= \int -\log \frac{p(\x|\z) p(\z | c) p(c)}{q(\z | \x, c)}dq \\
= \int - \log p(\x | \z)dq + \int \log \frac{q(\z | \x, c)}{p(\z | c)}dq
+ \log(k) \\
= \int - \log p(\x | \z)dq + KL(q(\z | \x, c) \| p(\z | c)) + \text{const}
\end{aligned}
\end{equation}


Since the $\z$ prior depends on the category, $p(\z | c)$, it should be some
sort of "blobs" mixture type of distribution.
The inference model is just $q(\z | \x,c)$. The regularization kl-term tries
to impose $q(\z | \x, c)$ to be close to $p(z | c)$, so if all works well 
$q(\z | \x, c)$ should look like a mixture distribution ("blobs").

Now for concrete choice of distribution families:
$p(c)$ is already chosen for us as uniform categorical. 
For the rest we again use diagonal Gaussians.
$p(\z | c)$ will be parametrized by an encoder network taking only the
categorical information. Essentially this network will map each category into
some "blob" around some centroid in the latent space. 
$p(\x | \z)$ describes how given $\z$ it defines a diagonal normal distribution
with fixed (or restricted) variance back in the
observed space like the decoder network in the vanilla case.
$q(\z | \x, c)$ means that in this case the encoder takes as input both $\x$ and
$c$ and defines a diagonal Gaussian in the latent space.
The difference is that with this model after we train it, the encoder will
encode a mixture distribution in $\z$, we will get several blobs in the latent
space corresponding to the classes.

From equation~\ref{eq:cvaeloss}, we can ignore the constant and see that the
reconstruction term that will make sure the decoder reconstruct the image in
our example, while the kl-term imposes a mixture distribution in the latent
space.

Finally there are circumstances that we use CVAE to "forget" the categories
rather then to encode them by setting a fixed prior $p(\z | c) \equiv p(\z)$.
An example for such use-case is if we have for example several batches of data,
for the same type of data but from different experiment. In this case we can use
a CVAE model with fixed prior to reduce the batch effect.

%MEMO: add examples with CVAE on MNIST. one with learned prior for mixture
%latent, one with fixed prior for batch effect reduction, one vanilla VAE.
%plots of the umaps...

\begin{figure}[!h]
\begin{framed}
\centering
\includegraphics[width=0.4 \textwidth]{plots/cvae.gv.png}
\caption{Graphical model of the CVAE with a learned prior $p(\z | c)$.
as usual the solid arrows depict the inference model (encoder) and the dotted
ones the generative model (decoder)}
\label{fig:cvae}
\end{framed}
\end{figure}

Figure~\ref{fig:cvaeumap} shows in the left a umap plot of the latent space for
the CVAE model which we described first, on the MNIST data set.
It uses the learned prior $p(\z|c)$ and
$\x$ and $c$ are conditionally independent given $\z$.
The middle image shows the reconstruction of the clusters centers (the means of
$p(\z | c)$). In this model the encoder completely separates the categories in
the latent representation, resulting in distinct blobs.
The right image is the CVAE described last, the one which "forgets" the
category, with fixed prior $p(\z | c) = p(\z) = \NN(\z;0,1)$.
In this model $\x$ is not conditionally independent from $c$, but instead it 
is directly dependent on both $\z$ and $c$ ($p(\x | \z, c)$).
The result is an encoding which mixes a lot of the categories in the latent
space but we can still notice some clustering.

\begin{figure}[!h]
\begin{framed}
\centering
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{images/cvae2lp.umap.mnist.png}
\caption{learned prior}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\includegraphics[width=\textwidth]{images/cvae2lp.clusterheads.mnist.png}
\caption{Reconstruction of the cluster centers}
\end{subfigure}
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[width=\textwidth]{images/cvae1nolp.umap.mnist.png}
\caption{no learned prior}
\end{subfigure}
\caption{CVAE, different use cases}
\label{fig:cvaeumap}
\end{framed}
\end{figure}

\chapter{Gaussian mixture model VAEs}
%Theoretical background and 
%with some examples from publications and my own tests.

\section{Motivation}

\begin{figure}[!hb]
\begin{framed}
\centering
\includegraphics[width=0.5\textwidth]{plots/dirichlet_gmm.gv.png}
\caption{Dirichlet GMM model}
\label{fig:dirgmm}
\end{framed}
\end{figure}

\begin{figure}[!hb]
\begin{framed}
\centering
\includegraphics[width=0.5\textwidth]{plots/dirichlet_gmm_cvae.gv.png}
\caption{Dirichlet GMM conditional model}
\label{fig:dirgmmcvae}
\end{framed}
\end{figure}


\begin{figure}
\begin{framed}
\centering
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{images/model_mnist_10c_generation.png}
\caption{}
%\caption{blabla}
%\label{bla}
\end{subfigure}
%\includegraphics[width=0.4\linewidth]{images/model_mnist_10c_generation.png}
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{images/model_mnist_10c_umap.png}
\caption{}
%\includegraphics[width=0.4\linewidth]{images/model_mnist_10c_umap.png}
\end{subfigure}
\caption{a figure}
\label{fig:myfig}
\end{framed}
\end{figure}

\subsection{Relation between AE and VAE}

\subsection{Conditional VAE}

\chapter{Experiments and results}
\section{Tests with MNIST and FMNIST}

\section{Tests with scRNAseq Data}
some words about (sc)RNAseq and published papers where 
AE and VAE models have been applied.
What we were hoping to achieve and compare with.

\chapter{Discussion, some remarks and conclusions}
punkt.
punkt.
punkt.





%\author{Yiftach Kolb}
%\date{Berlin, \today}
%\maketitle

%\section*{Intro Foo}
%\lipsum{1}
%\section{Bar}
%\lipsum{2}
%\appendix
%\section{Spam}
%\lipsum{3}

\printbibliography

\end{document}


