% https://de.overleaf.com/learn/latex/How_to_Write_a_Thesis_in_LaTeX_(Part_1)%3A_Basic_Structure
% https://en.wikibooks.org/wiki/LaTeX/Document_Structure
%\documentclass[11pt, twoside]{report}
\documentclass[11pt, a4paper]{report}
%\documentclass[a4paper,11pt]{scrartcl}
\usepackage{syntonly}
%\syntaxonly
\usepackage[utf8]{inputenc}
\usepackage[backend=biber]{biblatex}
\addbibresource{mybib.bib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{float}
\graphicspath{ {images/} }
\usepackage{blindtext}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{IEEEtrantools}
%\usepackage{showframe}
%\usepackage[a4paper, width=150mm, top=5mm, bottom=5mm]{geometry}
\usepackage[a4paper, width=150mm, top=25mm, bottom=25mm]{geometry}
\usepackage{fancyhdr}
%\setlength{\headheight}{12pt}
%\setlength{\headheight}{15.2pt}
%\pagestyle{empty}
%\pagestyle{myheadings}
%\pagestyle{headings}
%\pagestyle{plain}
\pagestyle{fancy}
\fancyhf{}
%\fancyhead[L]{\leftmark:}
%\fancyhead[L]{\chapter~\thechapter:}
%\fancyhead[L]{\thechapter}
\fancyhead[C]{\rightmark}
%\fancyhead[R]{\thesection}
\fancyfoot[C]{\thepage}

\setlength{\parskip}{1.0em}
\setlength{\parindent}{1em}

\usepackage{lipsum}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{mydef}{Definition}[chapter]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[chapter]


%newcommands
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\X}{\mathbf{X}}
%\newcommand{\B}{\{-1,1\}}
%\newcommand{\bvec}[1]{\mathbf{#1}}
%\newcommand{\bv}[1]{\mathbf{#1}}
%\newcommand{\b}[1]{\boldsymbol{#1}}
\newcommand{\bv}[1]{\boldsymbol{#1}}
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
\newcommand{\gt}{>}
\newcommand{\lt}{<}
\newcommand{\tuple}[1]{\langle #1 \rangle}

\begin{document}
%\pagestyle{fancy}
\begin{titlepage}
\begin{center}
{\includegraphics{images/MPIMG_RGB_gruen.png}}\\
\vspace*{1cm}
%\vfill
\Large
\textbf{List of topics to cover}
%\vfill

%\vspace{0.5cm}


%\normalsize
\large
With section titles and brief explantions.
\vfill
%\vspace{1.0cm}

Yiftach Kolb

Berlin, \today

\vfill
{\includegraphics{images/fu-logo_bildschirm_RGB1.jpg}}
\end{center}
\end{titlepage}

%\author{Kolb, Yiftach}
%\date{Berlin, \today}
%\title{Topic List}
%\maketitle

\chapter*{Abstract}
punkt.
punkt.
\nocite{guo2017improved}

\chapter*{Declaration}
punkt.
punkt.
\nocite{bishop2006pattern}
\nocite{serre2001matrices}
\nocite{kingma2013auto}
\nocite{lotfollahi2018generative}

\chapter*{Acknowledgement}
punkt.\cite{kingma2013auto}
punkt.

\listoftables

\listoffigures

\tableofcontents

\chapter{Introduction}
punkt.
\emph{punkt}, \textit{punkt}.

\chapter{Basic notations and definitions}
%\section{Basic Definitions}

Throughout this paper (modulo typing errors) we use capital bold math 
Latin or Greek letters ($\bv{X, \Sigma}$)
to represent matrices, bold small math letters ($\bv{x}$) represent (usually
row) vectors, but
in cases where it makes sense may also represent a matrices such as a batch
of several vectors (each row is a different data point).
Or another example,
$\bv{\sigma}$ may represent both
the covariance matrix and the variance vector of a diagonal Gaussian
distribution.
Non-bold math letters ($x, \sigma$, \dots) may represent scalar or vectors in some cases
and hopefully it is clear from the context or explicitly stated.


\section{The data}
we are assuming that the input data unless otherwise
stated is real and 2-dimensional. Its rows represent \emph{samples}, for
example---cells in the case of scRNAseq dataset, 
while its columns represent \emph{variables}, for example---genes in
scRNAseq dataset. 

In addition to $\bv{X}$ there may be additional data with information about
class or conditions. We use \emph{one-hot encoding} to represent such
information.


\begin{mydef}
\label{def:datamatrix}
A \emph{data matrix} is simply a real valued matrix $\bv{X} \in \R^{N \times n}$
which represent a set of $N$ $n$-dimensional data points.
The $N$ rows are also called \emph{observations} and the $n$ columns are
\emph{variables}.
\end{mydef}

\begin{mydef}
\label{def:classmatrix}
A \emph{class matrix}, or also \emph{condition matrix}
$\bv{C} \in \R^{\N \times c}$ is simply a real matrix which
represents one-hot encoding of $c$ classes or conditions over $N$ samples.
For example if sample $i$ has class $j$, then 
$(\forall k\in 1, \dots, c) \bv{C}[i,k] = \delta_{jk}$.

We say that that $\bv{C}$ is a \emph{class probability matrix} or \emph{relaxed
class matrix} (same with condition)
if instead of being one-hot it is a distribution matrix, namely each row is
non-negative and sums up to $1$.
\end{mydef}

Usually if the input data includes class/condition information, it comes as a
class matrix (pure one-hot) but the output (the prediction) is naturally 
probabilistic and hence is relaxed.

\chapter{Linear algebra preliminary}

\section{SVD and PCA}
Let $\bv{X} \in \R^{N \times n}$ be a real-valued matrix
representing $N$ samples of some
$n$-dimensional data points and let
$r= \text{rank}(X) \leq \min(n,N)$. 

$\X \X^T$ and $\X^T \X$ are both symmetric and positive semi-definite.
Their eigenvalues are non-negative, and they both have
the same positive 
eigenvalues, exactly $r$ such, which we mark
$s_1^2 \geq s_2^2 \geq \dots s_r^2 \gt 0$. The values
$s_1 \dots s_r$ are called the \emph{singular values} of $\bv{X}$.

Let $\bv{S} = 
\begin{pmatrix}
s_1 & & &\\
& s_2 & &\\
& & \ddots &\\
& & & s_r
\end{pmatrix} \in \R^{r \times r}
$

Let $\bv{U} \in \R^{N \times N}$ be the (column) eigenvectors of $\X \X^T$ sorted
by their eigenvalues. 
Then $\bv{U} = (\bv{U}_r, \bv{U}_k)$ where $\bv{U}_r \ \in \R^{N \times r}$ are the first $r$
eigenvectors corresponding to the non-zero eigenvalues, and $\bv{U}_k$ are the
eigenvectors corresponding to the $N-r$ $0$-eigenvalues.
Similarly, and let 
$\bv{V}  = (\bv{V}_r, \bv{V}_k)\in \R^{n \times n}$ be the (column) eigenvectors of $\X^T \X$, sorted
by the eigenvalues, where $\bv{V}_r \ \in \R^{n \times r}$ are the firs $r$
eigenvalues and $\bv{V}_k$ are the $n-r$ null-eigenvectors.

Then the \textit{singular value decomposition (SVD)} of $\X$ is 

\begin{equation}
\label{eq:svd}
\X = \bv{U} \bv{D} \bv{V}^T
\end{equation}
where 
$\bv{D} =
\left(
\begin{array}{c | c}
\bv{S} & \bv{0} \\
\hline
\bv{0} & \bv{0}
\end{array}
\right) \in \R^{N \times n}
$ is diagonal.


$\bv{V}_r$ are called the \textit{(right) principle components} of $\X$.
Note that $\bv{V}_r^T \bv{V}_r = \bv{I}_r$ and that 
$\X = \X \bv{V}_r \bv{V}_r^T$. If one looks at the second expression, 
it means that the each row of $\X$ is spanned by the orthogonal
basis $\bv{V}_r$ (because the other vectors of $\bv{V}$ are in $\text{ker}(\X)$.

More generally
For every $l \leq r$, let $\bv{V}_l \in \R^{N \times l}$ be the first $l$ components,
Then $\X\bv{V}_l \bv{V}_l^T$ is as close as we can get to $\X$ within an
$l$-dimensional subspace of $R^n$, and $\bv{V_l}$ minimizes

\begin{equation}
\label{eqn:pca}
\bv{V}_l = \text{argmin} \{
\|\X - \X \bv{W}\bv{W}^T\|_F^2 \quad : \quad \bv{W} \in \R^{n \times l}, \bv{W}^T \bv{W} =
\bv{I}_l\}
\end{equation} 

Where $\| \cdot \|_F^2$ is simply the sum of squares of the matrix' entries.

If we consider the more general
minimization problems: 

%\begin{subequations}
\begin{IEEEeqnarray}{C}
\label{eqn:pca2}
%\begin{aligned}
\min\{\|\X - \X \bv{E}\bv{D}\|_F^2 \quad : 
\quad \bv{E,D^T} \in \R^{n \times
l},\} \\
\min\{\|\X - \X \bv{W}\bv{W}^{\dagger}\|_F^2 \quad : 
\quad \bv{W} \in \R^{n \times
l},\}
%\end{aligned}
\end{IEEEeqnarray}
%\end{subequations} 

It can be shown~\cite{plaut2018principal} that the 
last two problems are equivalent
and that for any solution $E,D$ it must hold that 
$\bv{D}=\bv{E}^{\dagger}$. ($\bv{D}$ is the Moore-Penrose generalized
inverse of $\bv{E}$).
Moreover,
$\bv{V}_l$ still minimizes the general problem~\ref{eqn:pca2} and for every
solution $\bv{W}$, it must hold that $\text{span}\{\bv{W}\} =
\text{span}\{\bv{V}_l\}$ (but it isn't necessarily an orthogonal matrix).

\chapter{Autoencoders, variance inference and variational autoencoders}
\section{Some basic definitions and concepts}
%\begin{mydef}
%\label{def:datamatrix}
%A \textit{data matrix} is simply a real valued matrix $\bv{X} \in \R^{N \times n}$
%which represent a set of $N$ $n$-dimensional data points.
%The $N$ rows are also called \textit{observations} and the $n$ columns are
%\textit{variables}.
%For example $\bv{X}$ may represent $N$ cells over $n$ genes in the case of
%single-cells RNAseq data.
%\end{mydef}

\begin{mydef}
\label{def:NN}
A \emph{feed forward neural network} is simply a parameterized differentiable
map $\phi_{w} : \R^n \to \R^m$. $\phi_w(x)$ is differentiable both in its input
variable $x$ as well as in its parameters $w$, which is also called its
\emph{weights}.

For example each affine function has the
form $f:x \to a \cdot x + b$. $a$ and $b$ are its trainable parameters (weights).
The input is treated as fixed (the data we are trying to explain).

Normally $\phi$ is a sequence of compositions of
more simple functions. We call such more basic unit in a composition sequence 
a \emph{layer}.
Each layer is itself a composition, with exactly a
single affine map, followed by $0$ or more dimension preserving functions
such as normalization functions or activation functions.
An \emph{activation function}
is a real values non-linear function which is applied element-wise over the
input later. For example the sigmoid function and the ReLU (rectified linear
unit) are well-known and often used activation functions.
\end{mydef}

\begin{mydef}
\label{def:lossfunc}
Usually together with a neural network comes an associated differentiable
function which is the \textit{loss function} $\mathcal{L} : \R^m \to \R$.

Typically the loss function is additive on the dimension, meaning it has the
form $\mathcal{L}(\bv{x}) = \sum_{i=1}^m \psi(x_i)$

An example for such a loss function is the square error $x \to \frac{1}\|x\|_2^2$
\end{mydef}

\begin{mydef}
\label{def:batch}
Let $\bv{X} \in  \R^{N \times n}$ be a data matrix. A \emph{batch}
$\bv{x} \in \R^{b \times n}$ is any subset of $b$ rows of $\bv{X}$.

Batch $\bv{x} \in \R^{b \times n}$ represents a subset of $b$ samples out of the
total of $N$ samples in the dataset. The operations of $\phi$ and $\mathcal{L}$
\emph{extend naturally} to batches (note that in this case $\bv{x}$ is a matrix and
$\bv{x}_i$ is a vector representing a single sample of the batch)--- we collect
for $\phi$ and we average for $\mathcal{L}$, namely: $\phi(\bv{x}) =
(\phi(\bv{x}_i)_{i=1}^m \in \R^{b \times m}$, and 
$\mathcal{L}(x) = \frac{1}{b}\sum_{i=1}^b
\mathcal{L}(\bv{x}_i)$.

If $\mathcal{L}$ is the square error function $\| \cdot \|_2^2$ on vectors,
then its expansion to batches is $\frac{1}{b}\| \cdot \|_F^2$. The reason why we
sum and don't average over the dimensions will be cleared later when we get into
variational inference.
\end{mydef}


Training the neural network $\phi_w$ means finding the weights that minimize the
loss function applied on the training set $X$, in other words minimizing
$\min_{w} (\mathcal{L}(\phi_w(X))$.

During a training step the network is applied on a batches $x$. Then the loss
function is applied at the output and a gradient (with relation to the weights)
is taken. This gradient is used for the weight update rule, which varies
depending on the specific training algorithm. Typical training algorithms are
SGD (stochastic gradient decent) and Adam, which is the one used throughout this
work.

We only need to define the network, the loss function and the specific training
algorithm. The rest (derivation, weight update etc.) is taken care for us by
the backend of the software (Pytorch) and can be regarded as a black box.



\section{Autoencoders}
\begin{mydef}
\label{def:autoencoder}
An \textit{Autoencoder} (AE) is a neural network $\phi = D \circ E$ with a "bottleneck" layer 
and which
approximates the identity function on the training input.

We call $E$ which projects into the bottleneck, the \emph{encoder}, and $D$
which expands back into the input dimensions, the \emph{decoder}.
\end{mydef}

%A typical loss function for the AE is usually the mean sum of squares (MSE)
%$\mathcal{L}(X;\phi) = \frac{1}{N}\|X - \phi(X)\|_F^2 = \frac{1}{N}\|X - D \circ
%E (X) \|_F^2$.

%Note that the MSE can be interpreted probabilitically, if we assume our input
%comes from a random diagonal Gaussian with constant unit variant, we can interpret
%$D(E(X)) = \mu_x$ and the mean square error is $\log \mathcal{N}(x ; mu_x, 1)$
%(up to some scaling factor). Where $\log \mathcal{N}(\cdot ; \mu_x, 1)$ is the 
%log-density function for diagonal Gaussian with mean $\mu_x$ and unit variance. 

\subsection{Relation between PCA and AE}
For centered data $X$ (every variable, column, has $0$ sample mean), the first
$k \leq R(X)$ principle components $P$ are the solution for
equation~\ref{eqn:pca},
whereas a \textbf{linear} autoencoder
solves equation~\ref{eqn:pca2}.
As mentioned,
it must hold that $E = D^{\dagger}$ (the encoder must be the Moore-Penrose
inverse of the decoder).

A linear autoencoder (an AE where $\phi$ is linear) is therefore almost equivalent to
PCA~\cite{plaut2018principal}, in that in the optimum, a bottleneck space of dimension $k$
is spanned by the first $k$ principle components of the input $X$.
In general, an AE can be seen a PCA-like, but non-linear method for
dimensionality reduction.

\section{Variational Inference}

Here we briefly explain the idea behind variational inference and introduce the
ELBO which is the loss function we'll use throughout this text.
For more details see~\fullcite{bishop2006pattern}.

We have a set of observation $X = \{x_1, \dots , x_N\}$ which we try to explain
by a probabilitic model. 
$X$ is called the data or the observation (or something similar). We assume that
the $x_i$'s are i.i.d with some distribution function $p(x)$ and therefore for
the entire dataset it holds that $p(X) = \prod x_i$.

We have some reason to believe that behind the scences there is some hidden
(latent) shot caller $Z = \{z_1 \dots z_N\}$ that determines the fate of $X$.
In other words we think that $X$ is conditioned on $Z$ and and we can speak of
the joint distribution $p(X,Z) = p(X|Z)p(Z)$ and all said distributions factor
over the individual samples multiplicatively, i.e.
$p(X|Z) = \prod p(z_i | x_i)$.

Suppose that we have a fully Bayesian model. In this case there are no
parameters because the parameters are themselves stochastic variables with some
suitable priors. We can therefore pack all the latent variables and stochastic
parameters into one latent "meta variable" $Z = (z_1, z_2, \dots )$, each $z_i$
is some multidimensional r.v and possibly composed of several simpler r.vs (for
example a categotical and a normal r.vs).
We similarly pack all the observed variables into one meta variable $X$.
Together we have a distribution $p(X,Z)$ and the working assumption is that it
is easy to factorize $p(X,Z) = p(X|Z)p(Z)$, however $p(Z|X)$ is intractable and
$p(X)$ is unknown.

We are being Bayesian here so we consider $X = (x_1, x_2, \dots)$ to really be
constant, a set of observation and we want to best explain $p(X)$ by finding as
high as possible lower boud (or rather to $\log p(X)$, the log evidence).
A second goal is to approximate the intractable $p(Z|X)$ by some simpler
distribution $q(Z)$ taken from some family of distributions.

The following equation finds a lower bound (elbo) for the log evidence.
(using Jensen's inequality)

\begin{equation}\label{eq:elbo}
\begin{aligned}
\log p(X) &= \log \int p(X,Z) dZ = \log \int \frac{p(X,Z)}{q(Z)} q(Z)dZ \\
&= \log \int \frac{p(X,Z)}{q(Z)}dq(Z) 
\geq \int \log \frac{p(X,Z)}{q(Z)}dq(Z) 
\triangleq \mathcal{L}(q)
\end{aligned}
\end{equation}

In equation~\ref{eq:elbo} we found a lower bound $\mathcal{L}(q)$ for the log
evidence $\log p(X)$, which we call "evidence lower bound" or elbo for short.
Whatever distribution $q$ we put in elbo will not be
greater than the real log evidence so we are looking for the $q$ which
maximizes it.

Now we show that maximizing actually obtains the log evidence and it is
equivalent to minimizing $KL(q(Z) \| p(Z|X)$:

\begin{equation}\label{eq:kl_bound}
\begin{aligned}
\mathcal{L}(q) &\triangleq \int \log \frac{p(X,Z)}{q(Z)} d q(Z)
= \int \log \frac{p(Z|X)p(X)}{q(Z)} d q(Z) \\
&= \int \log p(X) dq(Z) - \int \log \frac{q(Z)}{p(Z|X)} dq(Z) 
= \log p(X) - KL(q(Z) \| p(Z|X)
\end{aligned}
\end{equation}

We can rewrite equation~\ref{eq:kl_bound} as:
\begin{equation}\label{eq:elbokl}
\log p(X) = \mathcal{L}(q) - KL(q(Z) \| p(Z|X))
\end{equation}

Equation~\ref{eq:elbokl} shows the the elbo minus the kl-divergence are constant
and equal the log evidence. Therefore minimizing the kl-divergence (which is
always non-negative) simultaneously maximizes the elbo and vicer-versa.

\section{Variational Autoencoder}
\subsection{Adding parameters}

Our models will not be fully Bayesian, but rather parametrized.
In this case let $\theta$ represent the set of parameters for $p$, and $\phi$
the parameters for $q$. Meaning we are dealing with a family of distributions
$p_{\theta}(x,z)$ and another family $q_{\phi}(z)$.

For any $\theta$ and any $\phi$, the equations from the previous chapter hold
also in the parametrize form, i.e $\log p_{\theta}(x) = \mathcal{L}(q_{\phi}) -
KL(q_{\phi}(Z) \| p_{\theta}(Z|X)$.

We assume that we can only approach the "real" distribution using
$\theta$ from below $\log p(X) \geq \log p_{\theta}(X)$.
So together with equation~\ref{eq:elbo} we have

\begin{equation}\label{eq:parelbo}
\begin{aligned}
(\forall \theta, \phi)\log p(X) & \geq \log p_{\theta}(X) \geq \mathcal{L}(q_{\phi})
= \int \frac{p_{\theta}(Z|X)}{q_{\phi}(Z)} dq_{\phi}(Z)
\end{aligned}
\end{equation}

So from equation~\ref{eq:elbokl} we again see that by finding the parameters
$\phi$, $\theta$ that maximize the elbo we approach the real log evidence as much
as we can within the limits of the parametrized family of distributions we use.

\subsection{Using neural networks for the parametrization}
In this text we deal with variational autoencoders (VAE).
A VAE is a neural network which is used to define and optimize the parameters
$\phi$ and $\theta$.

Specifically the encoder part of the network is a non-linear map $f_{\theta}(Z)$ which is
used to define $P_{\theta}(X|Z)$. For example, we can assume that $P_{\theta}$
is a family of multivariate Gaussians and in this case $f_{\theta}(Z) =
(\mu(Z), \Sigma(Z))$. Meaning the encoder maps $Z$ to the location vector and
covariance matrice. The parameter $\theta$ in this case are the weights of the
encoder neural network. In parametrize the prior $p(Z)$ however in this case its
parameter is not a function of $X$. In practice there is no reason to do this
for most VAEs and we choose some simple fixed prior distribution for $p(Z)$.

The decoder network is similarly defined as a non-linear function $g_{\phi}(X)$ which
maps $X$ into the parameters defining the family $q_{\phi}(Z)$. Here too $\phi$
represent the weights of the decoder.

\remark{}\label{rem:abuse_of_notation}
In many papers about VAEs (including this text) the decoder is described as
$q_{\phi}(X | Z)$. This is an abuse of notation, because there is neither 
join distribution $q_{\phi}(X,Z)$ nor marginal $q_{\phi}(X)$ to speak of.
However $q_{\phi}(Z)$ is meant to approximate the intractable $p(Z|X)$ and
that's presumably the reason for the abused notation.


\subsection{Graphical representation}

%punkt.
%punkt.
%\remark{My remark}[See \cite{makhzani2015adversarial}] This is interesting
%\dots.
%punkt.

It is both convenient as well as informative to include a graphical description
of our probabilitic models by way of plate diagrams.

Please note that we drop the $\phi, \theta$ subscript but they are still there
in reality.

In a plate diagram nodes represent random variables and arrows
represent dependency.
Figure~\ref{fig:vae_model} is a plate diagram of the VAE model with slight
adaptation. We use doted arrows to represent the arrows of the inference model,
and regular arrows for the generative model. Regular (triangular) arrowhead
represents real probabilitic dependency whereas rounded arrows are reminding us
that this is not a real probabilitic dependency (recall
\ref{rem:abuse_of_notation}) which maybe we can call 'parametric
dependency'.

Plate represents packing of $N$ i.i.ds since we have $N$ observations $X =
(x_i)_1^N$ and correspondingly $N$ latent variables $Z = (z_i)$.

Shaded node represent known values (either observation of prior).

The squared $\zeta$ node represent some fixed parameters which describes the
prior distribution of $P(z)$. Usually it is not shown in the papers about VAE
but we just wanted to remind the reader that it can be parametrize in general.

The generative model therefore factors as: $p(x,z) = p(x|z)p(z|\zeta) =
p(x|z)p(z)$

The inference model in this case is just $q(z)$ but we might denote is as
$q(z|x)$ because it tries to approximate $p(z|x)$.

Note that the graphical model has no assumption about the specific types of
distributions involved (Gaussian, Dirichlet or whateve \dots) and that is left
for the actual implementation.

In the case of a "vanilla" VAE, We chose the prior to be diagonal standard Gaussian
$p(z) \sim N(;0,1)$. And $p(z | x)$ is then chosed as diagonal Gaussian (whose
means and variances we find by training the network).

\begin{figure}
\centering
\begin{subfigure}[b]{0.2\textwidth}
\includegraphics[width=\textwidth]{../plots/vae_p.gv.png}
\caption{generative model ($p(x,z)$}
%\caption{blabla}
%\label{bla}
\end{subfigure}
%\includegraphics[width=0.4\linewidth]{images/model_mnist_10c_generation.png}
\begin{subfigure}[b]{0.2\textwidth}
\includegraphics[width=\textwidth]{../plots/vae_q.gv.png}
\caption{inference model ($q(z|x)$)}
%\includegraphics[width=0.4\linewidth]{images/model_mnist_10c_umap.png}
\end{subfigure}
\begin{subfigure}[b]{0.2\textwidth}
\includegraphics[width=\textwidth]{../plots/vae.gv.png}
\caption{the combined graphical model}
%\includegraphics[width=0.4\linewidth]{images/model_mnist_10c_umap.png}
\end{subfigure}
\caption{VAE graphical model}
\label{fig:vae_model}
\end{figure}

\chapter{Gaussian mixture model VAEs}

Theoretical background and 
with some examples from publications and my own tests.


\begin{figure}
\centering
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{images/model_mnist_10c_generation.png}
\caption{}
%\caption{blabla}
%\label{bla}
\end{subfigure}
%\includegraphics[width=0.4\linewidth]{images/model_mnist_10c_generation.png}
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{images/model_mnist_10c_umap.png}
\caption{}
%\includegraphics[width=0.4\linewidth]{images/model_mnist_10c_umap.png}
\end{subfigure}
\caption{a figure}
\label{fig:myfig}
\end{figure}

\subsection{Relation between AE and VAE}

\subsection{Conditional VAE}

\chapter{Experiments and results}
\section{Tests with MNIST and FMNIST}

\section{Tests with scRNAseq Data}
some words about (sc)RNAseq and published papers where 
AE and VAE models have been applied.
What we were hoping to achieve and compare with.

\chapter{Discussion, some remarks and conclusions}
punkt.
punkt.
punkt.





%\author{Yiftach Kolb}
%\date{Berlin, \today}
%\maketitle

%\section*{Intro Foo}
%\lipsum{1}
%\section{Bar}
%\lipsum{2}
%\appendix
%\section{Spam}
%\lipsum{3}

\printbibliography

\end{document}


